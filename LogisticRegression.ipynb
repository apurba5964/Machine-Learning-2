{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "maxAcc = 0.0\n",
    "maxIter = 0\n",
    "C_Lambda = 0.03\n",
    "TrainingPercent = 80\n",
    "ValidationPercent = 10\n",
    "TestPercent = 10\n",
    "M = 10#5#10\n",
    "PHI = []\n",
    "IsSynthetic = False\n",
    "IsConcat = True\n",
    "humanDataSampleSize=791\n",
    "gscDataSampleSize=5000\n",
    "humanFeaturesFile='HumanObserved-Features-Data.csv'\n",
    "humanSamePairFile='same_pairs.csv'\n",
    "humanDiffPairFile='diffn_pairs.csv'\n",
    "gscFeatureFile = 'GSC-Features.csv'\n",
    "gscSamePairFile='same_pairs_gsc.csv'\n",
    "gscDiffPairFile='diffn_pairs_gsc.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processData(data,samePair,diffPair,sampleSize,isGSC):\n",
    "    \n",
    "    if(isGSC == False):\n",
    "        data = pd.read_csv(data,index_col=0)\n",
    "    else:\n",
    "        data = pd.read_csv(data)\n",
    "\n",
    "\n",
    "    samePair = pd.read_csv(samePair)\n",
    "\n",
    "\n",
    "    diffPair = pd.read_csv(diffPair)\n",
    "\n",
    "\n",
    "    merged1 = pd.merge(samePair, data, left_on = ['img_id_A'],right_on= ['img_id'],how = 'inner').drop(['img_id'], axis='columns')\n",
    "    merged2 = pd.merge(samePair, data, left_on = ['img_id_B'],right_on= ['img_id'],how = 'inner').drop(['img_id','img_id_A','img_id_B','target'], axis='columns')\n",
    "\n",
    "    merged3=pd.concat([merged1, merged2],axis=1)\n",
    "\n",
    "\n",
    "    merged4=merged1 - merged2\n",
    "    merged4=merged4.drop(['img_id_A','img_id_B','target'], axis='columns')\n",
    "    merged4=pd.concat([merged4,merged1['target']],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    diffmerged1 =pd.merge(diffPair, data, left_on = ['img_id_A'],right_on= ['img_id'],how = 'inner').drop(['img_id'], axis='columns')\n",
    "    diffmerged2 = pd.merge(diffPair, data, left_on = ['img_id_B'],right_on= ['img_id'],how = 'inner').drop(['img_id','img_id_A','img_id_B','target'], axis='columns')\n",
    "\n",
    "\n",
    "    diffmerged3=pd.concat([diffmerged1, diffmerged2],axis=1).sample(n=sampleSize)\n",
    "\n",
    "\n",
    "    diffmerged4=diffmerged1 - diffmerged2\n",
    "    diffmerged4=diffmerged4.drop(['img_id_A','img_id_B','target'], axis='columns')\n",
    "    diffmerged4=pd.concat([diffmerged4,diffmerged1['target']],axis=1)\n",
    "    diffmerged5=diffmerged4.sample(n=sampleSize).abs()\n",
    "\n",
    "\n",
    "    if(isGSC == True):\n",
    "        merged3=merged3.sample(n=sampleSize)\n",
    "        subtractionFeatures=merged4.sample(n=sampleSize).abs()\n",
    "    else:\n",
    "        merged3=merged3\n",
    "        subtractionFeatures=merged4.abs()\n",
    "\n",
    "    concatFeatures=merged3.append(diffmerged3)    \n",
    "    concatFeatures=shuffle(concatFeatures)\n",
    "    print(\"Feature Concatenation:\"+str(concatFeatures.shape))\n",
    "    subtractionFeatures=subtractionFeatures.append(diffmerged5)\n",
    "    subtractionFeatures=shuffle(subtractionFeatures)\n",
    "    print(\"Feature Subtraction:\"+str(subtractionFeatures.shape))\n",
    "    return concatFeatures,subtractionFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTargetVector(features):\n",
    "\n",
    "    target=features['target'].values.T.tolist()\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "def GenerateRawData(features,isConcat):\n",
    "    \n",
    "    #features = features.loc[:, (features != 0).any(axis=0)]\n",
    "    if(isConcat == True):\n",
    "        dataMatrix = features.drop(columns=['img_id_A','img_id_B','target']).as_matrix()\n",
    "    else:\n",
    "        dataMatrix = features.drop(columns=['target']).as_matrix()\n",
    "    dataMatrix = np.transpose(dataMatrix)\n",
    "    #print (dataMatrix)\n",
    "\n",
    "    return dataMatrix\n",
    "\n",
    "def GenerateTrainingTarget(rawTraining,TrainingPercent = 80):\n",
    "    TrainingLen = int(math.ceil(len(rawTraining)*(TrainingPercent*0.01)))\n",
    "    t           = rawTraining[:TrainingLen]\n",
    "    #print(str(TrainingPercent) + \"% Training Target Generated..\")\n",
    "    return t\n",
    "\n",
    "\n",
    "def GenerateTrainingDataMatrix(rawData, TrainingPercent = 80):\n",
    "    T_len = int(math.ceil(len(rawData[0])*0.01*TrainingPercent))\n",
    "    d2 = rawData[:,0:T_len]\n",
    "    #print(str(TrainingPercent) + \"% Training Data Generated..\")\n",
    "    return d2\n",
    "\n",
    "def GenerateValData(rawData, ValPercent, TrainingCount): \n",
    "    valSize = int(math.ceil(len(rawData[0])*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    dataMatrix = rawData[:,TrainingCount+1:V_End]\n",
    "    #print (str(ValPercent) + \"% Val Data Generated..\")  \n",
    "    return dataMatrix\n",
    "\n",
    "def GenerateValTargetVector(rawData, ValPercent, TrainingCount): \n",
    "    valSize = int(math.ceil(len(rawData)*ValPercent*0.01))\n",
    "    #print(rawData)\n",
    "    V_End = TrainingCount + valSize\n",
    "    t =rawData[TrainingCount+1:V_End]\n",
    "    #print (str(ValPercent) + \"% Val Target Data Generated..\")\n",
    "    #print(t)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFeatureData(isConcat,data,samePair,diffPair,sampleSize,isGSC):\n",
    "    concatFeatures,subtractionFeatures = processData(data,samePair,diffPair,sampleSize,isGSC)\n",
    "    return  concatFeatures,subtractionFeatures\n",
    "\n",
    "def generateData(features,isConcat):\n",
    "    RawTarget = GetTargetVector(features)\n",
    "    RawData   = GenerateRawData(features,isConcat)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    return  RawTarget,RawData  \n",
    "\n",
    "#Creating Training dataset\n",
    "def createTrainingDataset(RawTarget,RawData):\n",
    "    TrainingTarget = np.array(GenerateTrainingTarget(RawTarget,TrainingPercent))\n",
    "    TrainingData   = GenerateTrainingDataMatrix(RawData,TrainingPercent)\n",
    "    print(\"TrainingTarget Data: \"+str(TrainingTarget.shape))\n",
    "    print(\"Training Data: \"+str(TrainingData.shape))\n",
    "    return TrainingTarget,TrainingData\n",
    "\n",
    "\n",
    "#Prepare Validation data\n",
    "def createValidationDataset(RawTarget,RawData,TrainingTarget):\n",
    "    \n",
    "    ValDataAct = np.array(GenerateValTargetVector(RawTarget,ValidationPercent, (len(TrainingTarget))))\n",
    "    ValData    = GenerateValData(RawData,ValidationPercent, (len(TrainingTarget)))\n",
    "    print(\"ValidationTarget Data: \"+str(ValDataAct.shape))\n",
    "    print(\"Validation Data: \"+str(ValData.shape))\n",
    "    return ValDataAct,ValData\n",
    "\n",
    "#Prepare Testing data\n",
    "def createTestingDataset(RawTarget,RawData,TrainingTarget,ValDataAct):\n",
    "    \n",
    "    TestDataAct = np.array(GenerateValTargetVector(RawTarget,TestPercent, (len(TrainingTarget)+len(ValDataAct))))\n",
    "    TestData = GenerateValData(RawData,TestPercent, (len(TrainingTarget)+len(ValDataAct)))\n",
    "    print(\"TestingTarget Data: \"+str(TestDataAct.shape))\n",
    "    print(\"Testing Data: \"+str(TestData.shape))\n",
    "    return TestDataAct,TestData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(features):\n",
    "    return 1 / (1 + np.exp(-features))\n",
    "\n",
    "def GetAccuracy(VAL_TEST_OUT,ValDataAct):\n",
    "    sum = 0.0\n",
    "    t=0\n",
    "    accuracy = 0.0\n",
    "    counter = 0\n",
    "    val = 0.0\n",
    "    valPred = []\n",
    "    for i in range (0,len(VAL_TEST_OUT)):\n",
    "    \tvalPred.append(float(np.around(VAL_TEST_OUT[i], 0)))\n",
    "\n",
    "    \n",
    "    accuracy = accuracy_score(ValDataAct, valPred)\n",
    " \n",
    "    return (str(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Concatenation:(1582, 21)\n",
      "Feature Subtraction:(1582, 10)\n",
      "TrainingTarget Data: (1266,)\n",
      "Training Data: (18, 1266)\n",
      "ValidationTarget Data: (158,)\n",
      "Validation Data: (18, 158)\n",
      "TestingTarget Data: (157,)\n",
      "Testing Data: (18, 157)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ML_Tensor_Keras/lib/python3.6/site-packages/ipykernel_launcher.py:12: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#Feature Generation\n",
    "concatFeatures,subtractionFeatures=generateFeatureData(IsConcat,humanFeaturesFile,humanSamePairFile\n",
    "                                  ,humanDiffPairFile,humanDataSampleSize,False)\n",
    "#Feature Concatenation\n",
    "concatRawTarget,concatRawData = generateData(concatFeatures,IsConcat)\n",
    "concatTrainingTarget,concatTrainingData=createTrainingDataset(concatRawTarget,concatRawData)\n",
    "concatValidationTarget,concatValidationData=createValidationDataset(concatRawTarget,concatRawData,concatTrainingTarget)\n",
    "concatTestDataAct,concatTestData=createTestingDataset(concatRawTarget,concatRawData,concatTrainingTarget,concatValidationTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runLogisticRegression(featureType,datasetType,TrainingData,RawData,ValData,TestData,\n",
    "                        TrainingTarget,ValDataAct,TestDataAct):\n",
    "    \n",
    "    W_Now        = np.zeros(RawData.shape[0])\n",
    "    La           = 0.0001\n",
    "    learningRate = 0.5\n",
    "    L_Acc_Val   = []\n",
    "    L_Acc_TR    = []\n",
    "    L_Acc_Test  = []\n",
    "    W_Mat        = []\n",
    "    L_Acc_Test_Acc  = []\n",
    "    iteration = []\n",
    "\n",
    "    for i in range(0,400):\n",
    "        \n",
    "    \n",
    "        #print ('---------Iteration: ' + str(i) + '--------------')\n",
    "        pred = np.dot(np.transpose(W_Now),TrainingData)\n",
    "        a = sigmoid(pred)\n",
    "        error = a - TrainingTarget\n",
    "\n",
    "        Delta_W = np.multiply(TrainingData,error) \n",
    "\n",
    "        Delta_W_Mean=Delta_W.sum(axis=1) / RawData.shape[0]\n",
    "\n",
    "        Regularizer = np.dot(La,W_Now) / RawData.shape[0]\n",
    "        Delta_W_Mean_Reg = np.add(Delta_W_Mean,Regularizer) \n",
    "\n",
    "        x=np.dot(learningRate,Delta_W_Mean_Reg)\n",
    "\n",
    "        W_Next=W_Now - x\n",
    "\n",
    "        W_Now=W_Next\n",
    "\n",
    "        TR_TEST_OUT   = np.dot(np.transpose(W_Next),TrainingData)\n",
    "        a_test = sigmoid(TR_TEST_OUT)\n",
    "        Acc_TR       = GetAccuracy(a_test,TrainingTarget)\n",
    "        L_Acc_TR.append(float(Acc_TR.split(',')[0]))\n",
    "\n",
    "\n",
    "        VAL_TEST_OUT  = np.dot(np.transpose(W_Next),ValData)\n",
    "        v_test = sigmoid(VAL_TEST_OUT) \n",
    "        Acc_Val      = GetAccuracy(v_test,ValDataAct)\n",
    "        L_Acc_Val.append(float(Acc_Val.split(',')[0]))\n",
    "    \n",
    "        #-----------------TestingData Accuracy---------------------#\n",
    "        TEST_OUT      = np.dot(np.transpose(W_Next),TestData)\n",
    "        t_test = sigmoid(TEST_OUT)\n",
    "        #print(t_test)\n",
    "        Acc_Test = GetAccuracy(t_test,TestDataAct)\n",
    "        #L_Erms_Test.append(float(Erms_Test.split(',')[1]))\n",
    "        L_Acc_Test_Acc.append(float(Acc_Test.split(',')[0]))\n",
    "        if(i%5==0):\n",
    "            iteration.append(i)\n",
    "\n",
    "\n",
    "    print ('----------Gradient Descent Solution--------------------')\n",
    "    print ('UBITname      = APURBAMA')\n",
    "    print ('Person Number = 50288705')\n",
    "    print ('----------------------------------------------------')\n",
    "    print (\"------------------\"+str(datasetType)+\"----\" + str(featureType)+ \" Dataset------------------\")\n",
    "    print ('------------------Logistic Regression Model----------------------')\n",
    "    print (\" \\nLambda  = 0.0001 \\neta=0.5\")\n",
    "    print (\"Training Accuracy  = \" + str(np.around(max(L_Acc_TR),5)))\n",
    "    print (\"Validation Accuracy= \" + str(np.around(max(L_Acc_Val),5)))\n",
    "\n",
    "    print (\"Testing Accuracy    = \" + str(np.around(max(L_Acc_Test_Acc),5)))\n",
    "    \n",
    "    \n",
    " \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Gradient Descent Solution--------------------\n",
      "UBITname      = APURBAMA\n",
      "Person Number = 50288705\n",
      "----------------------------------------------------\n",
      "------------------Feature Concatenation----Human Dataset------------------\n",
      "------------------Logistic Regression Model----------------------\n",
      " \n",
      "Lambda  = 0.0001 \n",
      "eta=0.5\n",
      "Training Accuracy  = 52.05371\n",
      "Validation Accuracy= 55.6962\n",
      "Testing Accuracy    = 57.96178\n"
     ]
    }
   ],
   "source": [
    "#Running Logistic Regression on Human COncatenation Dataset\n",
    "runLogisticRegression('Human','Feature Concatenation',concatTrainingData,concatRawData,concatValidationData\n",
    "                    ,concatTestData,concatTrainingTarget,concatValidationTarget,concatTestDataAct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingTarget Data: (1266,)\n",
      "Training Data: (9, 1266)\n",
      "ValidationTarget Data: (158,)\n",
      "Validation Data: (9, 158)\n",
      "TestingTarget Data: (157,)\n",
      "Testing Data: (9, 157)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ML_Tensor_Keras/lib/python3.6/site-packages/ipykernel_launcher.py:14: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Feature Subtraction\n",
    "subRawTarget,subRawData = generateData(subtractionFeatures,False)\n",
    "subTrainingTarget,subTrainingData=createTrainingDataset(subRawTarget,subRawData)\n",
    "subValidationTarget,subValidationData=createValidationDataset(subRawTarget,subRawData,subTrainingTarget)\n",
    "subTestDataAct,subTestData=createTestingDataset(subRawTarget,subRawData,subTrainingTarget,subValidationTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Gradient Descent Solution--------------------\n",
      "UBITname      = APURBAMA\n",
      "Person Number = 50288705\n",
      "----------------------------------------------------\n",
      "------------------Feature Subtraction----Human Dataset------------------\n",
      "------------------Logistic Regression Model----------------------\n",
      " \n",
      "Lambda  = 0.0001 \n",
      "eta=0.5\n",
      "Training Accuracy  = 53.08057\n",
      "Validation Accuracy= 54.43038\n",
      "Testing Accuracy    = 56.6879\n"
     ]
    }
   ],
   "source": [
    "#Running Logistic Regression on Human Subtraction Dataset\n",
    "runLogisticRegression('Human','Feature Subtraction',subTrainingData,subRawData,subValidationData\n",
    "                    ,subTestData,subTrainingTarget,subValidationTarget,subTestDataAct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Concatenation:(10000, 1027)\n",
      "Feature Subtraction:(10000, 513)\n"
     ]
    }
   ],
   "source": [
    "#GSC Feature Generation\n",
    "gscConcatFeatures,gscSubtractionFeatures=generateFeatureData(IsConcat,gscFeatureFile,gscSamePairFile\n",
    "                                  ,gscDiffPairFile,gscDataSampleSize,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingTarget Data: (8000,)\n",
      "Training Data: (1024, 8000)\n",
      "ValidationTarget Data: (999,)\n",
      "Validation Data: (1024, 999)\n",
      "TestingTarget Data: (999,)\n",
      "Testing Data: (1024, 999)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ML_Tensor_Keras/lib/python3.6/site-packages/ipykernel_launcher.py:12: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#GSC Feature Concatenation\n",
    "gscConcatRawTarget,gscConcatRawData = generateData(gscConcatFeatures,IsConcat)\n",
    "gscConcatTrainingTarget,gscConcatTrainingData=createTrainingDataset(gscConcatRawTarget,gscConcatRawData)\n",
    "gscConcatValidationTarget,gscConcatValidationData=createValidationDataset(gscConcatRawTarget,gscConcatRawData,gscConcatTrainingTarget)\n",
    "gscConcatTestDataAct,gscConcatTestData=createTestingDataset(gscConcatRawTarget,gscConcatRawData,gscConcatTrainingTarget,gscConcatValidationTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Gradient Descent Solution--------------------\n",
      "UBITname      = APURBAMA\n",
      "Person Number = 50288705\n",
      "----------------------------------------------------\n",
      "------------------Feature Concatenation----GSC Dataset------------------\n",
      "------------------Logistic Regression Model----------------------\n",
      " \n",
      "Lambda  = 0.0001 \n",
      "eta=0.5\n",
      "Training Accuracy  = 53.025\n",
      "Validation Accuracy= 53.35335\n",
      "Testing Accuracy    = 52.65265\n"
     ]
    }
   ],
   "source": [
    "#Running Linear Regression on GSC Concatenation Dataset\n",
    "runLogisticRegression('GSC','Feature Concatenation',gscConcatTrainingData,gscConcatRawData,gscConcatValidationData\n",
    "                    ,gscConcatTestData,gscConcatTrainingTarget,gscConcatValidationTarget,gscConcatTestDataAct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingTarget Data: (8000,)\n",
      "Training Data: (512, 8000)\n",
      "ValidationTarget Data: (999,)\n",
      "Validation Data: (512, 999)\n",
      "TestingTarget Data: (999,)\n",
      "Testing Data: (512, 999)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ML_Tensor_Keras/lib/python3.6/site-packages/ipykernel_launcher.py:14: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#GSC Feature Subtraction\n",
    "gscSubRawTarget,gscSubRawData = generateData(gscSubtractionFeatures,False)\n",
    "gscSubTrainingTarget,gscSubTrainingData=createTrainingDataset(gscSubRawTarget,gscSubRawData)\n",
    "gscSubValidationTarget,gscSubValidationData=createValidationDataset(gscSubRawTarget,gscSubRawData,gscSubTrainingTarget)\n",
    "gscSubTestDataAct,gscSubTestData=createTestingDataset(gscSubRawTarget,gscSubRawData,gscSubTrainingTarget,gscSubValidationTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Gradient Descent Solution--------------------\n",
      "UBITname      = APURBAMA\n",
      "Person Number = 50288705\n",
      "----------------------------------------------------\n",
      "------------------Feature Subtraction----GSC Dataset------------------\n",
      "------------------Logistic Regression Model----------------------\n",
      " \n",
      "Lambda  = 0.0001 \n",
      "eta=0.5\n",
      "Training Accuracy  = 69.55\n",
      "Validation Accuracy= 69.16917\n",
      "Testing Accuracy    = 68.86887\n"
     ]
    }
   ],
   "source": [
    "#Running Linear Regression on GSC Subtraction Dataset\n",
    "runLogisticRegression('GSC','Feature Subtraction',gscSubTrainingData,gscSubRawData,gscSubValidationData\n",
    "                    ,gscSubTestData,gscSubTrainingTarget,gscSubValidationTarget,gscSubTestDataAct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
